import sys
import os

# A√±adir rutas de los paquetes locales
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(os.path.join(parent_dir, "preprocesamiento_texto"))
sys.path.append(os.path.join(parent_dir, "preprocesamiento_img"))

try:
    import preprocesamiento
    print("‚úÖ M√≥dulo preprocesamiento (texto) importado")
except ImportError as e:
    print(f"‚ùå Error importando preprocesamiento (texto): {e}")
    sys.exit(1)

try:
    import preprocesamiento_img
    print("‚úÖ M√≥dulo preprocesamiento_img importado")
except ImportError as e:
    print(f"‚ùå Error importando preprocesamiento_img: {e}")
    sys.exit(1)

# Importar variables del preprocesamiento de texto
try:
    titulos_texto = preprocesamiento.titulos_filtrados
    nombres_archivos = preprocesamiento.nombres_archivos
    descripciones_originales = preprocesamiento.descripciones  # Descripci√≥n original sin procesar
    print(f"‚úÖ Variables de texto importadas: {len(titulos_texto)} t√≠tulos")
except AttributeError as e:
    print(f"‚ùå Error accediendo a variables de preprocesamiento: {e}")
    sys.exit(1)

try:
    titulos_img = preprocesamiento_img.titulos_img
    titulos_originales_img = preprocesamiento_img.titulos
    print(f"‚úÖ Variables de im√°genes importadas: {len(titulos_img)} t√≠tulos")
except AttributeError as e:
    print(f"‚ùå Error accediendo a variables de preprocesamiento_img: {e}")
    sys.exit(1)

# Funci√≥n para normalizar t√≠tulos para comparaci√≥n
def normalizar_titulo(titulo_tokens):
    """Convierte lista de tokens a string normalizado"""
    return ' '.join(sorted(titulo_tokens)).lower()

# Crear diccionarios para mapear t√≠tulos
titulos_texto_map = {}
for i, titulo in enumerate(titulos_texto):
    titulo_norm = normalizar_titulo(titulo)
    titulos_texto_map[titulo_norm] = {
        'indice': i, 
        'titulo_original': preprocesamiento.titulos[i],
        'descripcion_original': descripciones_originales[i],  # Agregar descripci√≥n
        'tokens': titulo,
        'archivo': nombres_archivos[i]
    }

titulos_img_map = {}
for i, titulo in enumerate(titulos_img):
    titulo_norm = normalizar_titulo(titulo)
    titulos_img_map[titulo_norm] = {
        'indice': i,
        'titulo_original': titulos_originales_img[i], 
        'tokens': titulo
    }

# Encontrar coincidencias y crear dataset final
print("üîÑ Creando conexiones entre im√°genes y texto...")
dataset_conexion = []
for titulo_norm, datos_texto in titulos_texto_map.items():
    if titulo_norm in titulos_img_map:
        datos_img = titulos_img_map[titulo_norm]
        dataset_conexion.append({
            'titulo': datos_texto['titulo_original'],
            'descripcion': datos_texto['descripcion_original'],  # Agregar descripci√≥n
            'archivo_imagen': datos_texto['archivo'],
            'tokens_procesados': datos_texto['tokens'],
            'indice_texto': datos_texto['indice'],
            'indice_img': datos_img['indice']
        })

print(f"‚úÖ Conexiones creadas: {len(dataset_conexion)} coincidencias encontradas")
print(f"üìä Total t√≠tulos texto: {len(titulos_texto_map)}")
print(f"üìä Total t√≠tulos imagen: {len(titulos_img_map)}")
print("üéâ Conexi√≥n img-txt completada exitosamente")
